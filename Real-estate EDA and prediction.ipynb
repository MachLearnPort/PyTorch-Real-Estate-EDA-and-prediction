{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forcasting the sale price in residential real-estate in Toronto\n",
    "\n",
    "### Objective\n",
    "The objective of this notebook is to prepare and explore Toronto freehold housing data, in order to develop a ML model to predict residential housing sale and time on market. Market sales will be used to evaluate future earnings of real-estate mortgage brokers, which will be used in conjunction with human resource accounting (HRA) models for employee valuations [1]. In this notebook, we will prepare the data for ingestion into a Pytorch NN, conduct a brief EDA, and finally train a model.\n",
    "\n",
    "### Data and Data Collection:\n",
    "The data was manually scraped from MLS based on previously sold free-hold properties, in the Toronto region from 2006-2018, with housing prices between five-hundred thousand and fifty million. The final scraped dataset contained 49,220 samples, accross 299 features (i.e. address, brokerage, listing price, sale price, number of bedrooms etc.). Once uploaded, the dataset will be reduced for easier computation.\n",
    "\n",
    "[1] Flamholtz, E. G. (2012). Human resource accounting: Advances in concepts, methods and applications. Springer Science & Business Media.\n",
    "\n",
    "##### Execution times: \n",
    "The execution times for cells are denoted as: `Exec time = ## sec`, and based on the following specs: i5 (7th gen) processors, and 8 Gb ram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Exec time = 6.2 sec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "from datetime import date\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "# Current directory (make sure .csv file is in the CWD)\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Import data\n",
    "df = pd.read_csv(cwd+'\\Toronto_Real_estate_labeled.csv')\n",
    "print(df.shape)\n",
    "\n",
    "#Amount of missing data\n",
    "No_miss = df.isnull().sum().sum()\n",
    "print('Missing data:' , (No_miss/(df.shape[0]*df.shape[1]))*100, '%')\n",
    "\n",
    "#Data description\n",
    "df.describe()\n",
    "#df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Lets look a little closer at the data.\n",
    "\n",
    "As you can see there are considerable number of columns that have no value - we will remove those columns. We will also remove any rows that are missing values in `Sold Price` and `Days on Market` since they are our metrics of interest. Likewise, we will also remove columns that have >50% missing values and reduce the size of our dataframe to 5000 rows. We also dropped several columns that were confounding.\n",
    "\n",
    "`Exec time = 4.3 sec`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First lets reduce the frame size so its easier to compute\n",
    "df = df.sample(n=5000)\n",
    "\n",
    "# Note: May take time\n",
    "# Convert all dates using Pandas i.e. contract date, sold date, listing entry date, \n",
    "df = df.dropna(axis=1, how='all')\n",
    "df = df.dropna(subset=['Sold Price'])\n",
    "df = df.dropna(subset=['Days on Market'])\n",
    "\n",
    "df = df.dropna(thresh=0.5*len(df), axis=1)\n",
    "df = df.drop(columns=['#',\n",
    "                      'star',\n",
    "                      'shopping',\n",
    "                      'LSC',\n",
    "                      'Address',\n",
    "                      'MLS #',\n",
    "                      'Address.1',\n",
    "                      '% Listing Price',\n",
    "                      'Assessment Roll #',\n",
    "                      'Area', #All Toronto\n",
    "                      'Bedrooms Total', #Dulicate\n",
    "                      'Class', #All residential\n",
    "                      'Special Designation1',\n",
    "                      'Co-op Brokerage ', #removed since co-op brokerage is only known at time of sale and not during listing\n",
    "                      'Commission Co-op Brokerage',\n",
    "                      'Contact After Expired',\n",
    "                      'Country', #They were all canada\n",
    "                      'Directions/Cross Streets', #Were to variable for the dataset\n",
    "                      'Directload Flag',\n",
    "                      'Drive',\n",
    "                      'Duplicate Listing',\n",
    "                      'Expiry Date',\n",
    "                      'Extras',\n",
    "                      'Kitchens Total', #Duplicate\n",
    "                      'Last Update',\n",
    "                      'List Brokerage Fax #',\n",
    "                      'List Brokerage Phone #',\n",
    "                      ' Listing Entry Date',\n",
    "                      'Lot Size code',\n",
    "                      'Municipality', #All toronto\n",
    "                      'Postal Code', #convert to geo coord later on\n",
    "                      'Prior LSC',\n",
    "                      'Province',\n",
    "                      'Picture Timestamp', \n",
    "                      'Timestamp',\n",
    "                      'Remarks for Broker',\n",
    "                      'Remarks for Clients',\n",
    "                      'Sale/Lease', # all sales based on data collection technique\n",
    "                      'Rooms +', #Missing alot of values\n",
    "                      'Seller/Lanlord Name',\n",
    "                      'Seller Property Info Statement',\n",
    "                      'Sold Entry Date',\n",
    "                      'Sewers', #Almost all the same\n",
    "                      'Status', #Almost all unavalible\n",
    "                      'Tax year',\n",
    "                      'Water', #Almost all the same\n",
    "                      'Unavalible Date',\n",
    "                      'Legal Description: Pt Lt 79 Pl 2170 *',\n",
    "                      ])\n",
    "\n",
    "#Convert date feild to panda datetime format\n",
    "df['Contract Date'] = pd.to_datetime(df['Contract Date'], errors = 'coerce') #if error occurs it generates NaT\n",
    "df['Sold Date'] = pd.to_datetime(df['Sold Date'], errors = 'coerce')\n",
    "df['Close Date'] = pd.to_datetime(df['Close Date'], errors = 'coerce')\n",
    "\n",
    "print(df.shape)\n",
    "No_miss = df.isnull().sum().sum()\n",
    "print('Missing data:' , (No_miss/(df.shape[0]*df.shape[1]))*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets add some more features\n",
    "\n",
    "We have immediatly dropped 205 feature and reduced the missing valued from 52% to a managble 10%. Now that we have everything a little more clean, lets bring in some extra features regarding market conditions. The external data comes from the Federal Reserve Bank of St. Louis and provides monthly information on: Housing Price Index, Consumer Price Index, Interbank Rate, Interest Rates, Unemployment Rate, and Working Age Population - all of which we feel will affect real estate sales. \n",
    "\n",
    "`Exec time = 2.1 sec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import market Data\n",
    "df_CPI = pd.read_csv(cwd+'\\Macro data\\CPI.csv')\n",
    "df_HPI = pd.read_csv(cwd+'\\Macro data\\HPI.csv')\n",
    "df_BR = pd.read_csv(cwd+'\\Macro data\\Inter_rate.csv')\n",
    "df_IR = pd.read_csv(cwd+'\\Macro data\\Intrest_rates.csv')\n",
    "df_UR = pd.read_csv(cwd+'\\Macro data\\\\Unemployment_rate.csv')\n",
    "df_WP = pd.read_csv(cwd+'\\Macro data\\working_pop.csv')\n",
    "\n",
    "# Merge the dataset\n",
    "df_CPI['observation_date'] = pd.to_datetime(df_CPI['observation_date'], errors = 'coerce') #Convert to datetime\n",
    "df_CPI['Date'] = df_CPI.observation_date.dt.to_period('M') #Convert to Month year\n",
    "\n",
    "df_HPI['observation_date'] = pd.to_datetime(df_HPI['observation_date'], errors = 'coerce') #Convert to datetime\n",
    "df_HPI['Date'] = df_HPI.observation_date.dt.to_period('M') #Convert to Month year\n",
    "\n",
    "df_BR['observation_date'] = pd.to_datetime(df_BR['observation_date'], errors = 'coerce') #Convert to datetime\n",
    "df_BR['Date'] = df_BR.observation_date.dt.to_period('M') #Convert to Month year\n",
    "\n",
    "df_IR['observation_date'] = pd.to_datetime(df_IR['observation_date'], errors = 'coerce') #Convert to datetime\n",
    "df_IR['Date'] = df_IR.observation_date.dt.to_period('M') #Convert to Month year\n",
    "\n",
    "df_UR['observation_date'] = pd.to_datetime(df_UR['observation_date'], errors = 'coerce') #Convert to datetime\n",
    "df_UR['Date'] = df_UR.observation_date.dt.to_period('M') #Convert to Month year\n",
    "\n",
    "df_WP['observation_date'] = pd.to_datetime(df_WP['observation_date'], errors = 'coerce') #Convert to datetime\n",
    "df_WP['Date'] = df_WP.observation_date.dt.to_period('M') #Convert to Month year\n",
    "\n",
    "\n",
    "#Add date column to df based on sold date\n",
    "df['Date'] = df['Sold Date'].dt.to_period('M') #Convert to Month year\n",
    "\n",
    "#Merge all data frames and drop un-neccessary columns\n",
    "df = pd.merge(left=df, left_on='Date', right=df_CPI, right_on='Date', how='left')\n",
    "df = pd.merge(left=df, left_on='Date', right=df_HPI, right_on='Date', how='left')\n",
    "df = pd.merge(left=df, left_on='Date', right=df_BR, right_on='Date', how='left')\n",
    "df = pd.merge(left=df, left_on='Date', right=df_IR, right_on='Date', how='left')\n",
    "df = pd.merge(left=df, left_on='Date', right=df_UR, right_on='Date', how='left')\n",
    "df = pd.merge(left=df, left_on='Date', right=df_WP, right_on='Date', how='left')\n",
    "\n",
    "# Now convert the date columns in the dataframe to ordinals\n",
    "#df['Contract Date'] = df['Contract Date'].apply(date.toordinal)\n",
    "#df['Sold Date'] = df['Sold Date'].apply(date.toordinal)\n",
    "#df['Close Date'] = df['Close Date'].apply(date.toordinal)\n",
    "\n",
    "df = df.drop(columns=['Date',\n",
    "                     'observation_date_x',\n",
    "                     'observation_date_y',\n",
    "                     'observation_date',\n",
    "                      'Contract Date',\n",
    "                      'Sold Date',\n",
    "                      'Close Date',\n",
    "                     ])\n",
    "#print(df.columns.to_series().groupby(df.dtypes).groups)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Lets focus on Sold Price and Days on Market\n",
    "\n",
    "Our working dataset is now ready from some EDA, with 5000 samples across 94 features describing the property and housing market conditions. Now, lets focus on our metrics of interest: `Sold Price` and `Days on Market`, to understand their distributions\n",
    "\n",
    "`Exec time = 0.4 sec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms\n",
    "df['Sold Price'].describe()\n",
    "\n",
    "f, axes = plt.subplots(1, 2)\n",
    "\n",
    "#histogram\n",
    "sns.distplot(df['Sold Price'], ax=axes[0]);\n",
    "\n",
    "#skewness and kurtosis\n",
    "print(\"Skewness: %f\" % df['Sold Price'].skew())\n",
    "print(\"Kurtosis: %f\" % df['Sold Price'].kurt())\n",
    "\n",
    "#histogram\n",
    "sns.distplot(df['Days on Market'], ax=axes[1]);\n",
    "\n",
    "#skewness and kurtosis\n",
    "print(\"Skewness: %f\" % df['Days on Market'].skew())\n",
    "print(\"Kurtosis: %f\" % df['Days on Market'].kurt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does this tell us?\n",
    "\n",
    "The results show that both `Sold Price` and `Days on Market` deviate from a normal distribution with a high positive skewness and kurtosis (i.e. Ku >> 3 ). Lets add some constraints to the `Sold price` and `Days on Market` metrics and try to better normalize the data.\n",
    "\n",
    "`Exec time = 0.48 sec `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constrain and transform the data\n",
    "df = df[df['Sold Price'] < 1250000]\n",
    "df = df[df['Days on Market'] < 75]\n",
    "\n",
    "f, axes = plt.subplots(1, 2)\n",
    "\n",
    "#Revised histogram and normal probability plot\n",
    "sns.distplot(df['Sold Price'], fit=norm, ax=axes[0]);\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df['Sold Price'], plot=plt)\n",
    "\n",
    "#transformed histogram and normal probability plot\n",
    "sns.distplot(df['Days on Market'], fit=norm, ax=axes[1]);\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df['Days on Market'], plot=plt)\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Correlation matrix\n",
    "\n",
    "By adding the restrictions we reduced many of the outliers while still maintaining a relatively high number of samples. We will work with with this datset for now, but further refinment is needed in next iterations of this model/data. Now lets generate a corrlation matrix of the numerical values.\n",
    "\n",
    "`Exec time = 1.8 sec `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation matrix - ** Drop List Price, Original Price,\n",
    "corrmat = df.corr()\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "sns.heatmap(corrmat, vmax=.8, square=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### What does it mean?\n",
    "\n",
    "As expected, the `List Price` and `Sale Price` are highly correlated. Notably, there are several positive and negitive correlations occuring with respect to the market conditions. There are other minor correlations (i.e. `Bedroom No`-`Room No.`etc.), but for the most part, a nice correlation matrix. Again, lets focus on our `Sold Price` and `Days on Market` variables. Lets drill down further and look at the correlation matrices specific our varibles of interest: `Sold Price` and `Days on Market`.\n",
    "\n",
    "`Exec time = 0.9 sec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sold Price correlation matrix\n",
    "k = 10 #number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, 'Sold Price')['Sold Price'].index\n",
    "df_c = df[cols]\n",
    "cm = df_c.corr()\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()\n",
    "\n",
    "#Days on Market correlation matrix\n",
    "k = 10 #number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, 'Days on Market')['Days on Market'].index\n",
    "df_c = df[cols]\n",
    "cm = df_c.corr()\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### On to scatter\n",
    "\n",
    "The specific correlation matrix looks good for both vars wrt their top 10 correlations (tho, the `Original Price`, `Listing Price` and `Sold Price` look slightly problematic). Lets continue and look at some scatter plots for these top correlating vars.\n",
    "\n",
    "\n",
    "`Exec time = 5.4 sec for both Sold Price and Days on Market` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sold Price scatterplot\n",
    "print('Sold Price scatter plot:')\n",
    "k = 5 #number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, 'Sold Price')['Sold Price'].index\n",
    "df_s = df[cols].astype('float64')\n",
    "df_s.dropna(how='all') #to drop if any value in the row has a nan\n",
    "sns.set()\n",
    "sns.pairplot(df_s, diag_kind='kde', size = 2.5)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Days on Market scatter plot:')\n",
    "#Days on Market scatterplot\n",
    "k = 5 #number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, 'Days on Market')['Days on Market'].index\n",
    "df_s = df[cols].astype('float64')\n",
    "df_s.dropna(how='all') #to drop if any value in the row has a nan\n",
    "sns.set()\n",
    "sns.pairplot(df_s, diag_kind='kde', size = 2.5)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### So whats going on?\n",
    "\n",
    "Their is some clear trending among `List Price`, `Original Price` and `Sold Price`, and `Composite_HPI`, and realtively little trends with `Days on Market`. Looking at the correlation matrices and scatter plots will be the extent of our EDA. So with being said, lets prepare our data (i.e. one-hot encoding, normalization etc.) for Pytorch. \n",
    "\n",
    "`Exec time = 4.8 sec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce dataframe size to 1500 before creating database for training\n",
    "df = df[0:1500]\n",
    "\n",
    "#Remove spaces in names\n",
    "df.columns = df.columns.str.replace('\\s+', '_')\n",
    "\n",
    "#Export current dataset of combined data in its raw form for records\n",
    "df.to_csv(\"Toronto_Real_estate_cleaned.csv\")\n",
    "\n",
    "##Un normalized Pytorch export\n",
    "#df = pd.get_dummies(df)\n",
    "#df.to_csv(\"Toronto_Real_estate_cleaned_1hot.csv\")\n",
    "\n",
    "# Normalize all Float and Int datatypes\n",
    "df_n = df\n",
    "cols_to_norm = list(df.select_dtypes(exclude=['object']))\n",
    "df_n[cols_to_norm] = df_n[cols_to_norm].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "\n",
    "df_n.to_csv(\"Toronto_Real_estate_ML.csv\")\n",
    "\n",
    "#Rename all vars to remove naming conflicts   \n",
    "df_n = pd.get_dummies(df_n)\n",
    "df_n.columns = range(df_n.shape[1])\n",
    "df_n.to_csv(\"Toronto_Real_estate_ML_noNames.csv\")\n",
    "\n",
    "print(df.shape)\n",
    "print(df_n.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally we can play with Pytorch\n",
    "\n",
    "For familiarization purposes, we will only train a shallow MLP for`Sold Price` prediction, on a small subset of data for reduced computational loading. Futher refinement of the model will take place off-line, and include Days on Market prediction.\n",
    "\n",
    "`Exec time = 3.2 sec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "df = pd.read_csv(cwd+'\\Toronto_Real_estate_ML_noNames.csv')\n",
    "\n",
    "# Create tensors\n",
    "train = df\n",
    "print('Shape of the train data with all features:', train.shape)\n",
    "train = train.select_dtypes(exclude=['object'])\n",
    "train.fillna(0,inplace=True)\n",
    "\n",
    "# Drop Unnamed column from import\n",
    "train = train.drop(train.columns[train.columns.str.contains('unnamed',case = False)],axis = 1)\n",
    "\n",
    "#NOTE: Var 1 is Sold_Price and Var 5 is Days on Market \n",
    "\n",
    "col_train = list(train.columns)\n",
    "col_train_bis = list(train.columns)\n",
    "\n",
    "col_train_bis.remove('1') #1 represents Sold_Price after recoding\n",
    "\n",
    "mat_train = np.matrix(train)\n",
    "mat_new = np.matrix(train.drop('1', axis = 1)) #Just training data without sold price\n",
    "mat_y = np.array(train['1']).reshape((train.shape[0],1)) #Just the sold price by itself\n",
    "\n",
    "print(mat_new.shape)\n",
    "print(mat_y.shape)\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N = mat_new.shape[0]/50\n",
    "D_in = mat_new.shape[1] \n",
    "H = 10\n",
    "D_out = 1\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs, and wrap them in Variables.\n",
    "x = Variable(torch.Tensor(mat_new))\n",
    "y = Variable(torch.Tensor(mat_y), requires_grad=False)\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam; the optim package contains many other\n",
    "# optimization algoriths. The first argument to the Adam constructor tells the\n",
    "# optimizer which Variables it should update.\n",
    "learning_rate = 1e-4\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(100):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.data[0])\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable weights\n",
    "    # of the model)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conculsion\n",
    "\n",
    "In this notebook, we generated a clean dataset, performed some preliminary EDA and trained a simple NN model. Although the trained model has not been optimized or used for prediction, the assignment was a great learning exercise, and expanded my understanding of Pytorch (and tensorflow through my readings). This assignment is a valuable stepping stone for expanding my understanding and skills with ML models and theory, and I look forward to building increasingly complex/robust models using these skills."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
